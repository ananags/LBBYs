{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes neuronales: CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Descripción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las Redes Neuronales Convolucionales (CNN, por sus siglas en inglés) son un tipo especial de red neuronal que se ha demostrado ser extremadamente eficaz para el procesamiento de datos estructurados en forma de grillas, como imágenes, series temporales y datos de audio. Este modelo se inspira en el sistema visual de los seres humanos, lo que le permite identificar patrones jerárquicos de manera eficiente, desde características simples (como bordes y texturas) hasta patrones más complejos (como formas o objetos completos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este proyecto, el objetivo es aplicar una red neuronal convolucional para un problema de clasificación binaria. Utilizaremos las capacidades de las CNN para extraer características relevantes del conjunto de datos y realizar una clasificación precisa, optimizando tanto las métricas de precisión como de recall, especialmente cuando se trata de un conjunto de datos desbalanceado. Se busca que el modelo no solo sea preciso, sino que también sea capaz de manejar los datos desbalanceados de manera efectiva, garantizando una clasificación más robusta en ambas clases.\n",
    "\n",
    "Este enfoque de modelado con CNN puede ser útil para diversas aplicaciones, no solo en visión por computadora, sino también en dominios como la clasificación de textos, predicción de series temporales, y en cualquier escenario donde las relaciones espaciales y jerárquicas sean importantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- Implementación\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, Dropout, MaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos\n",
    "df = pd.read_csv(r\"C:\\Users\\Day\\Downloads\\LBBYs-main\\LBBYs-main\\data\\processed\\df_train.csv\")\n",
    "df_test= pd.read_csv(r\"C:\\Users\\Day\\Downloads\\LBBYs-main\\LBBYs-main\\data\\raw\\test_nolabel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balancear el dataset con undersampling\n",
    "df_majority = df[df[\"Accept\"] == 1]\n",
    "df_minority = df[df[\"Accept\"] == 0]\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                   replace=False,\n",
    "                                   n_samples=len(df_minority),\n",
    "                                   random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced = pd.concat([df_majority_downsampled, df_minority])\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar X e y\n",
    "X = df_balanced.drop('Accept', axis=1)\n",
    "y = df_balanced['Accept']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_features = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_columns = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificar texto si hay columnas categóricas\n",
    "le = LabelEncoder()\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in categorical_columns:\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns_test = df_test.select_dtypes(include=['object']).columns\n",
    "\n",
    "categorical_columns_test = [col for col in categorical_columns_test if col != 'id']\n",
    "for col in categorical_columns_test:\n",
    "    if col in categorical_columns:  # Solo transformar las columnas que están en df_train\n",
    "        # Reemplaza las categorías desconocidas con 'Unknown' si es necesario\n",
    "        if 'Unknown' not in le.classes_:\n",
    "            le.classes_ = np.append(le.classes_, 'Unknown')\n",
    "        \n",
    "        # Reemplaza las categorías desconocidas con 'Unknown' y aplica la transformación\n",
    "        df_test[col] = df_test[col].astype(str).apply(lambda x: x if x in le.classes_ else 'Unknown')\n",
    "        df_test[col] = le.transform(df_test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns_test = df_test_features.select_dtypes(include=['object']).columns\n",
    "le = LabelEncoder()\n",
    "\n",
    "for col in categorical_columns_test:\n",
    "    df_test_features[col] = le.fit_transform(df_test_features[col].astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputar valores faltantes\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "df_test_imputed = imputer.fit_transform(df_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalar\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "df_test_scaled = scaler.fit_transform(df_test_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir en train/test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot select an axis to squeeze out which has size not equal to one",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Reshape para CNN (necesita 3D input: samples, time steps, features)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Luego expandimos la dimensión solo para los conjuntos de entrenamiento y test\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msqueeze\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1571\u001b[0m, in \u001b[0;36msqueeze\u001b[1;34m(a, axis)\u001b[0m\n\u001b[0;32m   1569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m squeeze()\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot select an axis to squeeze out which has size not equal to one"
     ]
    }
   ],
   "source": [
    "# Reshape para CNN (necesita 3D input: samples, time steps, features)\n",
    "# Luego expandimos la dimensión solo para los conjuntos de entrenamiento y test\n",
    "X_train = np.squeeze(X_train, axis=-1)  # Expande las dimensiones a (n_samples, n_features, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de X_train: (6129, 20)\n",
      "Dimensiones de df_test_scaled: (3284, 20)\n"
     ]
    }
   ],
   "source": [
    "# Comprobar las dimensiones de X_train y df_test_scaled\n",
    "print(\"Dimensiones de X_train:\", X_train.shape)\n",
    "print(\"Dimensiones de df_test_scaled:\", df_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de X_train: (6129, 20, 1, 1)\n",
      "Dimensiones de df_test_scaled: (3284, 20)\n"
     ]
    }
   ],
   "source": [
    "# Comprobar las dimensiones de X_train y df_test_scaled\n",
    "print(\"Dimensiones de X_train:\", X_train.shape)\n",
    "print(\"Dimensiones de df_test_scaled:\", df_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.- Evaluación y optimización\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=(X_train.shape[1], 1)),  # Capa convolucional\n",
    "    MaxPooling1D(pool_size=2),  # Capa de MaxPooling\n",
    "    Dropout(0.3),  # Dropout para evitar sobreajuste\n",
    "    Flatten(),  # Aplanar las características\n",
    "    Dense(128, activation='relu'),  # Capa densa con activación ReLU\n",
    "    Dropout(0.3),  # Dropout para evitar sobreajuste\n",
    "    Dense(1, activation='sigmoid')  # Capa de salida con activación sigmoide para clasificación binaria\n",
    "])\n",
    "\n",
    "# Compilar el modelo con la función de pérdida binary_crossentropy\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_3 (Conv1D)           (None, 16, 64)            384       \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, 8, 64)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 8, 64)             0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,177\n",
      "Trainable params: 66,177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6350 - accuracy: 0.6433 - val_loss: 0.6294 - val_accuracy: 0.6370\n",
      "Epoch 2/30\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.6172 - accuracy: 0.6635 - val_loss: 0.6131 - val_accuracy: 0.6599\n",
      "Epoch 3/30\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6064 - accuracy: 0.6741 - val_loss: 0.5980 - val_accuracy: 0.6762\n",
      "Epoch 4/30\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.6012 - accuracy: 0.6757 - val_loss: 0.5935 - val_accuracy: 0.6794\n",
      "Epoch 5/30\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.5948 - accuracy: 0.6808 - val_loss: 0.5968 - val_accuracy: 0.6688\n",
      "Epoch 6/30\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.5942 - accuracy: 0.6824 - val_loss: 0.5925 - val_accuracy: 0.6737\n",
      "Epoch 7/30\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.5868 - accuracy: 0.6873 - val_loss: 0.5864 - val_accuracy: 0.6819\n",
      "Epoch 8/30\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5857 - accuracy: 0.6902 - val_loss: 0.5831 - val_accuracy: 0.6909\n",
      "Epoch 9/30\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.5866 - accuracy: 0.6939 - val_loss: 0.5846 - val_accuracy: 0.6909\n",
      "Epoch 10/30\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.5802 - accuracy: 0.6971 - val_loss: 0.5829 - val_accuracy: 0.6900\n",
      "Epoch 11/30\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.5799 - accuracy: 0.6943 - val_loss: 0.5810 - val_accuracy: 0.6917\n",
      "Epoch 12/30\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.5749 - accuracy: 0.6983 - val_loss: 0.5868 - val_accuracy: 0.6860\n",
      "Epoch 13/30\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.5748 - accuracy: 0.6963 - val_loss: 0.5809 - val_accuracy: 0.6868\n",
      "Epoch 14/30\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.5731 - accuracy: 0.6969 - val_loss: 0.5851 - val_accuracy: 0.6884\n",
      "Epoch 15/30\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.5735 - accuracy: 0.7000 - val_loss: 0.5824 - val_accuracy: 0.6868\n",
      "Epoch 16/30\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.5711 - accuracy: 0.6998 - val_loss: 0.5894 - val_accuracy: 0.6786\n",
      "Epoch 17/30\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.5733 - accuracy: 0.6988 - val_loss: 0.5788 - val_accuracy: 0.6909\n",
      "Epoch 18/30\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5705 - accuracy: 0.7037 - val_loss: 0.5862 - val_accuracy: 0.6884\n",
      "Epoch 19/30\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5681 - accuracy: 0.7045 - val_loss: 0.5799 - val_accuracy: 0.6884\n",
      "Epoch 20/30\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.5693 - accuracy: 0.7081 - val_loss: 0.5740 - val_accuracy: 0.6941\n",
      "Epoch 21/30\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.5655 - accuracy: 0.7034 - val_loss: 0.5736 - val_accuracy: 0.6941\n",
      "Epoch 22/30\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.5627 - accuracy: 0.7024 - val_loss: 0.5796 - val_accuracy: 0.6852\n",
      "Epoch 23/30\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.5623 - accuracy: 0.7073 - val_loss: 0.5797 - val_accuracy: 0.6941\n",
      "Epoch 24/30\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.5629 - accuracy: 0.7024 - val_loss: 0.5757 - val_accuracy: 0.6933\n",
      "Epoch 25/30\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5579 - accuracy: 0.7094 - val_loss: 0.5716 - val_accuracy: 0.6917\n",
      "Epoch 26/30\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5519 - accuracy: 0.7132 - val_loss: 0.5738 - val_accuracy: 0.6974\n",
      "Epoch 27/30\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.5619 - accuracy: 0.7077 - val_loss: 0.5738 - val_accuracy: 0.6982\n",
      "Epoch 28/30\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.5541 - accuracy: 0.7106 - val_loss: 0.5759 - val_accuracy: 0.6990\n",
      "Epoch 29/30\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.5566 - accuracy: 0.7179 - val_loss: 0.5813 - val_accuracy: 0.6884\n",
      "Epoch 30/30\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.5549 - accuracy: 0.7083 - val_loss: 0.5728 - val_accuracy: 0.7039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19d2b5b4cd0>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predecir probabilidades\n",
    "y_pred= model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = model.predict(X_test)\n",
    "\n",
    "y_pred = (y_pred_prob > 0.4).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de X_train: (6129, 20)\n",
      "Dimensiones de df_test_scaled: (3284, 20)\n"
     ]
    }
   ],
   "source": [
    "# Comprobar las dimensiones de X_train y df_test_scaled\n",
    "print(\"Dimensiones de X_train:\", X_train.shape)\n",
    "print(\"Dimensiones de df_test_scaled:\", df_test_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Hacer predicciones\n",
    "y_test_pred = model.predict(df_test_scaled)\n",
    "y_test_bin = (y_test_pred > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de df_test_scaled: (3284, 20)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensiones de df_test_scaled:\", df_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[414 329]\n",
      " [159 631]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.56      0.63       743\n",
      "           1       0.66      0.80      0.72       790\n",
      "\n",
      "    accuracy                           0.68      1533\n",
      "   macro avg       0.69      0.68      0.68      1533\n",
      "weighted avg       0.69      0.68      0.68      1533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluar\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id  ACCEPT\n",
      "0  6b7ce9ec161       0\n",
      "1  96123015731       1\n",
      "2  b2c5181ac5b       0\n",
      "3  e6cb54a9e6a       1\n",
      "4  8eddf83466e       1\n"
     ]
    }
   ],
   "source": [
    "# Crear DataFrame con las predicciones (id y Accept)\n",
    "df_result = df_test[['id']].copy()  # Agregar 'id' del conjunto de test\n",
    "df_result['ACCEPT'] = y_test_bin\n",
    "# Verifica las primeras filas del DataFrame\n",
    "print(df_result.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Predicciones completas y archivo guardado como 'resultado_test.csv'!\n"
     ]
    }
   ],
   "source": [
    "# Guardar el resultado en un archivo CSV\n",
    "df_result.to_csv(\"SUBMMIT_CNN.csv\", index=False)\n",
    "\n",
    "print(\"¡Predicciones completas y archivo guardado como 'resultado_test.csv'!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3284, 2)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de df_test_scaled: (3284, 20)\n",
      "Dimensiones de y_test_pred: (3284, 1)\n"
     ]
    }
   ],
   "source": [
    "# Verifica las dimensiones de df_test_scaled y y_test_pred\n",
    "print(f\"Dimensiones de df_test_scaled: {df_test_scaled.shape}\")\n",
    "print(f\"Dimensiones de y_test_pred: {y_test_pred.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.- Conclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "El modelo muestra un rendimiento razonable en la clasificación de préstamos bancarios, con una precisión y recall aceptables para ambas clases (rechazados y aprobados). El modelo tiene una mayor capacidad para identificar correctamente los casos de rechazo (con un buen recall de la clase 0), pero presenta un ligero desequilibrio en la identificación de préstamos aprobados, con un recall algo más bajo para la clase 1.\n",
    "\n",
    "Aunque la exactitud global es del 68%, lo que indica que el modelo está clasificando correctamente una porción considerable de los casos, el desempeño podría mejorar, especialmente en la identificación de la clase minoritaria (préstamos aprobados). Los resultados sugieren que, aunque el modelo es funcional, se podrían realizar ajustes para mejorar el balance y la precisión, como ajustar el umbral de decisión o explorar otras técnicas de modelado."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
