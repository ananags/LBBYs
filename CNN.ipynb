{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes neuronales: CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Descripción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las Redes Neuronales Convolucionales (CNN, por sus siglas en inglés) son un tipo especial de red neuronal que se ha demostrado ser extremadamente eficaz para el procesamiento de datos estructurados en forma de grillas, como imágenes, series temporales y datos de audio. Este modelo se inspira en el sistema visual de los seres humanos, lo que le permite identificar patrones jerárquicos de manera eficiente, desde características simples (como bordes y texturas) hasta patrones más complejos (como formas o objetos completos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este proyecto, el objetivo es aplicar una red neuronal convolucional para un problema de clasificación binaria. Utilizaremos las capacidades de las CNN para extraer características relevantes del conjunto de datos y realizar una clasificación precisa, optimizando tanto las métricas de precisión como de recall, especialmente cuando se trata de un conjunto de datos desbalanceado. Se busca que el modelo no solo sea preciso, sino que también sea capaz de manejar los datos desbalanceados de manera efectiva, garantizando una clasificación más robusta en ambas clases.\n",
    "\n",
    "Este enfoque de modelado con CNN puede ser útil para diversas aplicaciones, no solo en visión por computadora, sino también en dominios como la clasificación de textos, predicción de series temporales, y en cualquier escenario donde las relaciones espaciales y jerárquicas sean importantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- Implementación\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Importar las librerías necesarias\n",
    "#Están incluidas librerías que fueron usadas como prueba\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, Dropout, MaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos\n",
    "df = pd.read_csv(r\"C:\\Users\\Day\\Downloads\\LBBYs-main\\LBBYs-main\\data\\processed\\new\\train_balanced_processed.csv\")\n",
    "df_test= pd.read_csv(r\"C:\\Users\\Day\\Downloads\\LBBYs-main\\LBBYs-main\\data\\processed\\new\\test_nolabel_processed.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balancear el dataset con undersampling\n",
    "df_majority = df[df[\"Accept\"] == 1]\n",
    "df_minority = df[df[\"Accept\"] == 0]\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                   replace=False,\n",
    "                                   n_samples=len(df_minority),\n",
    "                                   random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced = pd.concat([df_majority_downsampled, df_minority])\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar X e y\n",
    "X = df_balanced.drop('Accept', axis=1)\n",
    "y = df_balanced['Accept']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_features = df_test.drop(columns=['id'], errors='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_columns = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificar texto si hay columnas categóricas\n",
    "le = LabelEncoder()\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in categorical_columns:\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns_test = df_test.select_dtypes(include=['object']).columns\n",
    "\n",
    "categorical_columns_test = [col for col in categorical_columns_test if col != 'id']\n",
    "for col in categorical_columns_test:\n",
    "    if col in categorical_columns:  # Solo transformar las columnas que están en df_train\n",
    "        # Reemplazar las categorías desconocidas con 'Unknown' si es necesario\n",
    "        if 'Unknown' not in le.classes_:\n",
    "            le.classes_ = np.append(le.classes_, 'Unknown')\n",
    "        \n",
    "        # Reemplazar las categorías desconocidas con 'Unknown' y aplica la transformación\n",
    "        df_test[col] = df_test[col].astype(str).apply(lambda x: x if x in le.classes_ else 'Unknown')\n",
    "        df_test[col] = le.transform(df_test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns_test = df_test_features.select_dtypes(include=['object']).columns\n",
    "le = LabelEncoder()\n",
    "\n",
    "for col in categorical_columns_test:\n",
    "    df_test_features[col] = le.fit_transform(df_test_features[col].astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputar valores faltantes\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "df_test_imputed = imputer.fit_transform(df_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalar\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "df_test_scaled = scaler.fit_transform(df_test_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir en train/test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id  NewExist  UrbanRural  RevLineCr  LowDoc  BankStateInOhio  \\\n",
      "0  6b7ce9ec161         1           2          1       0                0   \n",
      "1  96123015731         1           1          1       0                1   \n",
      "2  b2c5181ac5b         1           2          1       0                0   \n",
      "3  e6cb54a9e6a         2           1          0       0                0   \n",
      "4  8eddf83466e         1           0          0       0                1   \n",
      "\n",
      "   ApprovalDateMonth  ApprovalFYGrouped  NoEmpGrouped  CreateJobBinary  \\\n",
      "0                 10               2005             0                0   \n",
      "1                  4               2000             1                1   \n",
      "2                  1               2003             1                0   \n",
      "3                  7               2004             1                1   \n",
      "4                 11               1991             1                1   \n",
      "\n",
      "   RetainedJobBinary  IsFranchise  DisbursementGrossGrouped  \n",
      "0                  1            0                         1  \n",
      "1                  1            0                         1  \n",
      "2                  1            0                         0  \n",
      "3                  1            0                         1  \n",
      "4                  1            0                         1  \n"
     ]
    }
   ],
   "source": [
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de X_train: (4681, 12)\n",
      "Dimensiones de df_test_scaled: (3284, 12)\n"
     ]
    }
   ],
   "source": [
    "# Comprobar las dimensiones de X_train y df_test_scaled\n",
    "print(\"Dimensiones de X_train:\", X_train.shape)\n",
    "print(\"Dimensiones de df_test_scaled:\", df_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de X_train: (4681, 12)\n",
      "Dimensiones de df_test_scaled: (3284, 12)\n"
     ]
    }
   ],
   "source": [
    "# Comprobar las dimensiones de X_train y df_test_scaled\n",
    "print(\"Dimensiones de X_train:\", X_train.shape)\n",
    "print(\"Dimensiones de df_test_scaled:\", df_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.- Evaluación y optimización\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear y entrenar el modelo MLP\n",
    "#A lo largo del entrenamiento se fueron modificando las diferentes características del modelo para buscar optimizarlo.\n",
    "model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=(X_train.shape[1], 1)),  # Capa convolucional\n",
    "    MaxPooling1D(pool_size=2),  # Capa de MaxPooling\n",
    "    Dropout(0.3),  # Dropout para evitar sobreajuste\n",
    "    Flatten(),  # Aplanar las características\n",
    "    Dense(128, activation='relu'),  # Capa densa con activación ReLU\n",
    "    Dropout(0.3),  # Dropout para evitar sobreajuste\n",
    "    Dense(1, activation='sigmoid')  # Capa de salida con activación sigmoide para clasificación binaria\n",
    "])\n",
    "\n",
    "# Compilar el modelo con la función de pérdida binary_crossentropy\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_1 (Conv1D)           (None, 8, 64)             384       \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 4, 64)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4, 64)             0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,409\n",
      "Trainable params: 33,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "117/117 [==============================] - 3s 16ms/step - loss: 0.6661 - accuracy: 0.5967 - val_loss: 0.6409 - val_accuracy: 0.6350\n",
      "Epoch 2/30\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.6517 - accuracy: 0.6213 - val_loss: 0.6323 - val_accuracy: 0.6446\n",
      "Epoch 3/30\n",
      "117/117 [==============================] - 2s 13ms/step - loss: 0.6424 - accuracy: 0.6263 - val_loss: 0.6317 - val_accuracy: 0.6286\n",
      "Epoch 4/30\n",
      "117/117 [==============================] - 1s 11ms/step - loss: 0.6401 - accuracy: 0.6207 - val_loss: 0.6328 - val_accuracy: 0.6222\n",
      "Epoch 5/30\n",
      "117/117 [==============================] - 1s 9ms/step - loss: 0.6365 - accuracy: 0.6341 - val_loss: 0.6314 - val_accuracy: 0.6329\n",
      "Epoch 6/30\n",
      "117/117 [==============================] - 1s 10ms/step - loss: 0.6363 - accuracy: 0.6319 - val_loss: 0.6288 - val_accuracy: 0.6329\n",
      "Epoch 7/30\n",
      "117/117 [==============================] - 1s 10ms/step - loss: 0.6314 - accuracy: 0.6381 - val_loss: 0.6274 - val_accuracy: 0.6339\n",
      "Epoch 8/30\n",
      "117/117 [==============================] - 1s 11ms/step - loss: 0.6315 - accuracy: 0.6408 - val_loss: 0.6282 - val_accuracy: 0.6425\n",
      "Epoch 9/30\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.6290 - accuracy: 0.6384 - val_loss: 0.6323 - val_accuracy: 0.6307\n",
      "Epoch 10/30\n",
      "117/117 [==============================] - 1s 11ms/step - loss: 0.6246 - accuracy: 0.6485 - val_loss: 0.6291 - val_accuracy: 0.6222\n",
      "Epoch 11/30\n",
      "117/117 [==============================] - 1s 11ms/step - loss: 0.6257 - accuracy: 0.6330 - val_loss: 0.6327 - val_accuracy: 0.6190\n",
      "Epoch 12/30\n",
      "117/117 [==============================] - 1s 11ms/step - loss: 0.6253 - accuracy: 0.6389 - val_loss: 0.6316 - val_accuracy: 0.6393\n",
      "Epoch 13/30\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.6271 - accuracy: 0.6402 - val_loss: 0.6292 - val_accuracy: 0.6275\n",
      "Epoch 14/30\n",
      "117/117 [==============================] - 1s 10ms/step - loss: 0.6176 - accuracy: 0.6469 - val_loss: 0.6250 - val_accuracy: 0.6403\n",
      "Epoch 15/30\n",
      "117/117 [==============================] - 1s 11ms/step - loss: 0.6190 - accuracy: 0.6461 - val_loss: 0.6265 - val_accuracy: 0.6393\n",
      "Epoch 16/30\n",
      "117/117 [==============================] - 1s 11ms/step - loss: 0.6209 - accuracy: 0.6501 - val_loss: 0.6256 - val_accuracy: 0.6382\n",
      "Epoch 17/30\n",
      "117/117 [==============================] - 1s 11ms/step - loss: 0.6194 - accuracy: 0.6530 - val_loss: 0.6239 - val_accuracy: 0.6425\n",
      "Epoch 18/30\n",
      "117/117 [==============================] - 1s 10ms/step - loss: 0.6183 - accuracy: 0.6562 - val_loss: 0.6242 - val_accuracy: 0.6425\n",
      "Epoch 19/30\n",
      "117/117 [==============================] - 1s 10ms/step - loss: 0.6180 - accuracy: 0.6565 - val_loss: 0.6250 - val_accuracy: 0.6425\n",
      "Epoch 20/30\n",
      "117/117 [==============================] - 1s 11ms/step - loss: 0.6150 - accuracy: 0.6621 - val_loss: 0.6267 - val_accuracy: 0.6435\n",
      "Epoch 21/30\n",
      "117/117 [==============================] - 1s 10ms/step - loss: 0.6144 - accuracy: 0.6568 - val_loss: 0.6238 - val_accuracy: 0.6393\n",
      "Epoch 22/30\n",
      "117/117 [==============================] - 1s 10ms/step - loss: 0.6151 - accuracy: 0.6608 - val_loss: 0.6202 - val_accuracy: 0.6361\n",
      "Epoch 23/30\n",
      "117/117 [==============================] - 1s 11ms/step - loss: 0.6149 - accuracy: 0.6493 - val_loss: 0.6205 - val_accuracy: 0.6393\n",
      "Epoch 24/30\n",
      "117/117 [==============================] - 1s 11ms/step - loss: 0.6138 - accuracy: 0.6496 - val_loss: 0.6220 - val_accuracy: 0.6382\n",
      "Epoch 25/30\n",
      "117/117 [==============================] - 1s 11ms/step - loss: 0.6074 - accuracy: 0.6533 - val_loss: 0.6260 - val_accuracy: 0.6403\n",
      "Epoch 26/30\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.6100 - accuracy: 0.6557 - val_loss: 0.6205 - val_accuracy: 0.6446\n",
      "Epoch 27/30\n",
      "117/117 [==============================] - 1s 11ms/step - loss: 0.6040 - accuracy: 0.6672 - val_loss: 0.6247 - val_accuracy: 0.6393\n",
      "Epoch 28/30\n",
      "117/117 [==============================] - 1s 10ms/step - loss: 0.6100 - accuracy: 0.6565 - val_loss: 0.6209 - val_accuracy: 0.6425\n",
      "Epoch 29/30\n",
      "117/117 [==============================] - 1s 11ms/step - loss: 0.6061 - accuracy: 0.6611 - val_loss: 0.6232 - val_accuracy: 0.6361\n",
      "Epoch 30/30\n",
      "117/117 [==============================] - 1s 10ms/step - loss: 0.6043 - accuracy: 0.6693 - val_loss: 0.6282 - val_accuracy: 0.6243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x207ea5c7a60>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de X_train: (4681, 12)\n",
      "Dimensiones de df_test_scaled: (3284, 12)\n"
     ]
    }
   ],
   "source": [
    "# Comprobar las dimensiones de X_train y df_test_scaled\n",
    "print(\"Dimensiones de X_train:\", X_train.shape)\n",
    "print(\"Dimensiones de df_test_scaled:\", df_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predecir probabilidades\n",
    "y_pred= model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = model.predict(X_test)\n",
    "\n",
    "y_pred = (y_pred_prob > 0.4).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de X_train: (4681, 12)\n",
      "Dimensiones de df_test_scaled: (3284, 12)\n"
     ]
    }
   ],
   "source": [
    "# Comprobar las dimensiones de X_train y df_test_scaled\n",
    "print(\"Dimensiones de X_train:\", X_train.shape)\n",
    "print(\"Dimensiones de df_test_scaled:\", df_test_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Hacer predicciones\n",
    "y_test_pred = model.predict(df_test_scaled)\n",
    "y_test_bin = (y_test_pred > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de df_test_scaled: (3284, 12)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensiones de df_test_scaled:\", df_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[311 278]\n",
      " [143 439]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.53      0.60       589\n",
      "           1       0.61      0.75      0.68       582\n",
      "\n",
      "    accuracy                           0.64      1171\n",
      "   macro avg       0.65      0.64      0.64      1171\n",
      "weighted avg       0.65      0.64      0.64      1171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluar\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id  ACCEPT\n",
      "0  6b7ce9ec161       0\n",
      "1  96123015731       1\n",
      "2  b2c5181ac5b       0\n",
      "3  e6cb54a9e6a       0\n",
      "4  8eddf83466e       1\n"
     ]
    }
   ],
   "source": [
    "# Crear DataFrame con las predicciones (id y Accept)\n",
    "df_result = df_test[['id']].copy()  # Agregar 'id' del conjunto de test\n",
    "df_result['ACCEPT'] = y_test_bin\n",
    "print(df_result.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Predicciones completas y archivo guardado como 'resultado_test.csv'!\n"
     ]
    }
   ],
   "source": [
    "# Guardar el resultado en un archivo CSV\n",
    "df_result.to_csv(\"SUBMMIT_CNN.csv\", index=False)\n",
    "\n",
    "print(\"¡Predicciones completas y archivo guardado como 'resultado_test.csv'!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3284, 2)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de df_test_scaled: (3284, 12)\n",
      "Dimensiones de y_test_pred: (3284, 1)\n"
     ]
    }
   ],
   "source": [
    "# Verificar las dimensiones de df_test_scaled y y_test_pred\n",
    "print(f\"Dimensiones de df_test_scaled: {df_test_scaled.shape}\")\n",
    "print(f\"Dimensiones de y_test_pred: {y_test_pred.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.- Conclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "El modelo basado en redes neuronales convolucionales (CNN) alcanzó una exactitud global del 63% en la predicción de decisiones sobre préstamos bancarios, lo que indica un desempeño moderado en términos generales. El modelo mostró un comportamiento desigual entre las clases: tuvo un mejor desempeño al identificar los préstamos aprobados (clase 1), con un recall del 79%, lo que sugiere una alta capacidad para detectar correctamente a los solicitantes aceptados.\n",
    "\n",
    "Sin embargo, la capacidad del modelo para identificar préstamos rechazados (clase 0) fue más limitada, con un recall del 47%, lo que indica una proporción considerable de falsos negativos en esta clase. Este desequilibrio se refleja también en las métricas de f1-score: 0.68 para la clase aprobada y 0.56 para la clase rechazada.\n",
    "\n",
    "En resumen, aunque el modelo CNN ofrece un rendimiento aceptable y es particularmente eficaz para detectar casos positivos (préstamos aprobados), sería recomendable implementar estrategias para mejorar la detección de préstamos rechazados. Entre las posibles acciones se incluyen el ajuste de umbrales de decisión, el uso de técnicas de balanceo de clases o la evaluación de arquitecturas más adecuadas para datos tabulares."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
